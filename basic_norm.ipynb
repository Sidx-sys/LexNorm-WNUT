{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Retreiving Data from Previous Analysis"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import pickle\r\n",
    "import numpy as np"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "conversion_map = {}\r\n",
    "freq_map = {}\r\n",
    "one_to_n_map = {}\r\n",
    "\r\n",
    "with open('utils/conversion_map.pkl', 'rb') as fp:\r\n",
    "    conversion_map = pickle.load(fp)\r\n",
    "\r\n",
    "with open('utils/freq_map.pkl', 'rb') as fp:\r\n",
    "    freq_map = pickle.load(fp)\r\n",
    "\r\n",
    "with open('utils/one_to_n_map.pkl', 'rb') as fp:\r\n",
    "    one_to_n_map = pickle.load(fp)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "freq_map[:10]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('u', 271),\n",
       " ('im', 148),\n",
       " ('dont', 72),\n",
       " ('nigga', 49),\n",
       " ('niggas', 45),\n",
       " ('n', 41),\n",
       " ('pls', 35),\n",
       " ('ur', 35),\n",
       " ('lil', 32),\n",
       " ('thats', 30)]"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Testing GLOVE Twitter Embeddings 50D"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "from gensim.scripts.glove2word2vec import glove2word2vec\r\n",
    "\r\n",
    "'''creating standard glove vectors for the given dataset'''\r\n",
    "glove_filename_std = '../glove_vectors/glove.6B.50d.txt'\r\n",
    "word2vec_filename_std = glove_filename_std + '.word2vec'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "glove2word2vec(glove_filename_std, word2vec_filename_std)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-16-018eeed9b299>:6: DeprecationWarning: Call to deprecated `glove2word2vec` (KeyedVectors.load_word2vec_format(.., binary=False, no_header=True) loads GLoVE text vectors.).\n",
      "  glove2word2vec(glove_filename_std, word2vec_filename_std)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(400000, 50)"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "'''creating twitter glove vectors for the given dataset'''\r\n",
    "glove_filename_twt = '../glove_vectors/glove.twitter.27B.50d.txt'\r\n",
    "word2vec_filename_twt = glove_filename_twt + '.word2vec'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "glove2word2vec(glove_filename_twt, word2vec_filename_twt)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-17-3bcfc541b522>:4: DeprecationWarning: Call to deprecated `glove2word2vec` (KeyedVectors.load_word2vec_format(.., binary=False, no_header=True) loads GLoVE text vectors.).\n",
      "  glove2word2vec(glove_filename_twt, word2vec_filename_twt)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(1193514, 50)"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "from gensim.models import KeyedVectors"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "vectorizer_std = KeyedVectors.load_word2vec_format(word2vec_filename_std, binary=False)\r\n",
    "print('You: ', vectorizer_std.get_vector('you'))\r\n",
    "\r\n",
    "vectorizer_twt = KeyedVectors.load_word2vec_format(word2vec_filename_twt, binary=False)\r\n",
    "print('u: ', vectorizer_twt.get_vector('u'))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "You:  [-1.0919e-03  3.3324e-01  3.5743e-01 -5.4041e-01  8.2032e-01 -4.9391e-01\n",
      " -3.2588e-01  1.9972e-03 -2.3829e-01  3.5554e-01 -6.0655e-01  9.8932e-01\n",
      " -2.1786e-01  1.1236e-01  1.1494e+00  7.3284e-01  5.1182e-01  2.9287e-01\n",
      "  2.8388e-01 -1.3590e+00 -3.7951e-01  5.0943e-01  7.0710e-01  6.2941e-01\n",
      "  1.0534e+00 -2.1756e+00 -1.3204e+00  4.0001e-01  1.5741e+00 -1.6600e+00\n",
      "  3.7721e+00  8.6949e-01 -8.0439e-01  1.8390e-01 -3.4332e-01  1.0714e-02\n",
      "  2.3969e-01  6.6748e-02  7.0117e-01 -7.3702e-01  2.0877e-01  1.1564e-01\n",
      " -1.5190e-01  8.5908e-01  2.2620e-01  1.6519e-01  3.6309e-01 -4.5697e-01\n",
      " -4.8969e-02  1.1316e+00]\n",
      "u:  [ 0.083004  1.0053    0.26507  -0.098562 -0.18409  -0.033368 -0.56497\n",
      "  0.43791   0.20716   1.288    -0.55683  -0.19672  -4.234    -0.063966\n",
      " -0.87323  -0.080305 -0.63516  -0.21967  -0.12525   1.0197    0.20205\n",
      "  0.19305   0.34885   1.0354    0.16417  -0.60181  -0.097535 -0.2351\n",
      "  0.1023    0.25122  -0.43261  -0.54675  -0.053332  0.56012   0.91886\n",
      "  0.19207   0.14778  -0.37423  -0.43194   0.67007  -2.5695    0.53407\n",
      "  0.65173  -0.15028  -0.18949  -0.31327   0.49442  -0.92346   0.25377\n",
      " -0.2268  ]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "type(vectorizer_twt.get_vector('u'))"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "def similarity(vec1, vec2):\r\n",
    "    return np.dot(vec1, vec2)/(np.linalg.norm(vec1)*np.linalg.norm(vec2))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "def comp_rs_ns(raw, norm):\r\n",
    "    try:\r\n",
    "        vec_norm_std = vectorizer_std.get_vector(norm)\r\n",
    "    except:\r\n",
    "        print(f\"Std vector not available for {norm}\")\r\n",
    "        return\r\n",
    "    try:\r\n",
    "        vec_raw_std = vectorizer_std.get_vector(raw)\r\n",
    "    except:\r\n",
    "        print(f\"Std vector not available for {raw}\")\r\n",
    "        return\r\n",
    "    print(f\"Similarity b/w norm std and raw std: {similarity(vec_norm_std, vec_raw_std)}\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "def comp_rt_nt(raw, norm):\r\n",
    "    try:\r\n",
    "        vec_norm_twt = vectorizer_twt.get_vector(norm)\r\n",
    "    except:\r\n",
    "        print(f\"twt vector not available for {norm}\")\r\n",
    "        return\r\n",
    "    try:\r\n",
    "        vec_raw_twt = vectorizer_twt.get_vector(raw)\r\n",
    "    except:\r\n",
    "        print(f\"Twt vector not available for {raw}\")\r\n",
    "        return\r\n",
    "    print(f\"Similarity b/w norm twt and raw twt: {similarity(vec_norm_twt, vec_raw_twt)}\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "for raw, freq in freq_map[:5]:\r\n",
    "    norm = list(conversion_map[raw])[0]\r\n",
    "    \r\n",
    "    print(f\"Raw: {raw}, Norm: {norm}\")\r\n",
    "    print(f\"Most Similar by std: {vectorizer_std.most_similar(raw)},\\ntwt: {vectorizer_twt.most_similar(raw)}\")\r\n",
    "    comp_rs_ns(raw, norm)\r\n",
    "    comp_rt_nt(raw, norm)\r\n",
    "    print()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Raw: u, Norm: you\n",
      "Most Similar by std: [('o', 0.7432858347892761), ('n', 0.7190415263175964), ('k', 0.7095504999160767), ('z', 0.7089523077011108), ('si', 0.7075910568237305), ('}', 0.6854987740516663), ('h', 0.6824235916137695), ('ne', 0.677496612071991), (']', 0.6725263595581055), ('ti', 0.6685909628868103)],\n",
      "twt: [('b', 0.9368361234664917), ('r', 0.9109147787094116), ('m', 0.8701779246330261), ('l', 0.867573082447052), ('f', 0.866772472858429), ('>', 0.8398881554603577), ('i', 0.8337498307228088), ('s', 0.8293094038963318), ('<', 0.8248172998428345), ('w', 0.8073067665100098)]\n",
      "Similarity b/w norm std and raw std: 0.4276062846183777\n",
      "Similarity b/w norm twt and raw twt: 0.7801663279533386\n",
      "\n",
      "Raw: im, Norm: i'm\n",
      "Most Similar by std: [('tirtzu', 0.7494168877601624), ('breisgau', 0.6679399609565735), ('ried', 0.6625338792800903), ('und', 0.6576429009437561), ('tz', 0.6549264788627625), ('der', 0.6433403491973877), ('ist', 0.6413055658340454), ('winkl', 0.6337300539016724), ('vmkabat', 0.631666362285614), ('haus', 0.6228776574134827)],\n",
      "twt: [(\"'m\", 0.8940563797950745), ('cuz', 0.8722367882728577), ('mad', 0.8689626455307007), ('thats', 0.8557406067848206), ('damn', 0.853709876537323), ('bad', 0.8486689329147339), ('guess', 0.848253071308136), ('gonna', 0.8429061770439148), ('i', 0.8427042961120605), ('know', 0.8419879674911499)]\n",
      "Std vector not available for i'm\n",
      "twt vector not available for i'm\n",
      "\n",
      "Raw: dont, Norm: don't\n",
      "Most Similar by std: [('trù', 0.7610262632369995), (\"'cause\", 0.7491459250450134), (\"y'\", 0.7481059432029724), ('kidding', 0.7344459295272827), ('geez', 0.7313922643661499), ('’ll', 0.709045946598053), ('gotta', 0.7040270566940308), ('wont', 0.690603494644165), ('okay', 0.6899604797363281), ('thats', 0.6882928609848022)],\n",
      "twt: [('cant', 0.9308992624282837), ('wont', 0.9169861674308777), ('know', 0.9121496081352234), (\"n't\", 0.911559522151947), ('cause', 0.9108346700668335), ('wanna', 0.9103906154632568), ('tell', 0.9101396799087524), ('talk', 0.9037295579910278), ('ca', 0.8902729153633118), ('mean', 0.8894258141517639)]\n",
      "Std vector not available for don't\n",
      "twt vector not available for don't\n",
      "\n",
      "Raw: nigga, Norm: nigger\n",
      "Most Similar by std: [('motherfucker', 0.8173415660858154), ('laat', 0.743552029132843), ('asshole', 0.736145555973053), ('haugr', 0.7221707701683044), ('spaz', 0.7119187712669373), ('f/v', 0.7100687623023987), ('ratón', 0.7092204093933105), ('padrão', 0.7047957181930542), ('palantír', 0.700185239315033), ('skank', 0.6979672908782959)],\n",
      "twt: [('bruh', 0.9266905188560486), ('bitch', 0.9227849841117859), ('dawg', 0.9215338230133057), ('lil', 0.9123411774635315), ('niggah', 0.9038726091384888), ('niggas', 0.9026524424552917), ('aint', 0.9017007946968079), ('homie', 0.8960006237030029), ('cuz', 0.894758939743042), ('shit', 0.8939277529716492)]\n",
      "Similarity b/w norm std and raw std: 0.6094620823860168\n",
      "Similarity b/w norm twt and raw twt: 0.7285074591636658\n",
      "\n",
      "Raw: niggas, Norm: niggers\n",
      "Most Similar by std: [('allez', 0.7316846251487732), (\"l'or\", 0.702649712562561), ('atrapado', 0.6786108016967773), ('orilla', 0.6690711379051208), ('tienda', 0.6688219308853149), ('ohh', 0.6645472645759583), ('comin', 0.6625955700874329), (\"l'écho\", 0.6614693403244019), ('danses', 0.6596717834472656), ('curva', 0.659233570098877)],\n",
      "twt: [('hoes', 0.9524016380310059), ('niggaz', 0.9504646062850952), ('bitches', 0.935326337814331), ('yall', 0.9287976622581482), ('nigga', 0.9026524424552917), ('mfs', 0.8963096737861633), ('aint', 0.8924072980880737), ('females', 0.8840673565864563), ('tryna', 0.8773045539855957), ('wit', 0.8701788783073425)]\n",
      "Similarity b/w norm std and raw std: 0.4238527715206146\n",
      "Similarity b/w norm twt and raw twt: 0.7278686761856079\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "def evaluate(raw, gold, pred, file_path, ignCaps=False, verbose=False):\r\n",
    "    cor = 0\r\n",
    "    changed = 0\r\n",
    "    total = 0\r\n",
    "    tp = 0\r\n",
    "    fp = 0\r\n",
    "    fn = 0\r\n",
    "    new_raws = 0\r\n",
    "    folder_path  = 'v1_verbose/'\r\n",
    "\r\n",
    "    if len(gold) != len(pred):\r\n",
    "        print('Error: gold normalization contains a different numer of sentences(' + str(len(gold)) + ') compared to system output(' + str(len(pred)) + ')')\r\n",
    "        return\r\n",
    "\r\n",
    "    for sentRaw, sentGold, sentPred in zip(raw, gold, pred):\r\n",
    "        if len(sentGold) != len(sentPred):\r\n",
    "            print('Error: a sentence has a different length in you output, check the order of the sentences')\r\n",
    "            return\r\n",
    "        for wordRaw, wordGold, wordPred in zip(sentRaw, sentGold, sentPred):\r\n",
    "            if ignCaps:\r\n",
    "                wordRaw = wordRaw.lower()\r\n",
    "                wordGold = wordGold.lower()\r\n",
    "                wordPred = wordPred.lower()\r\n",
    "            if wordRaw != wordGold:\r\n",
    "                changed += 1\r\n",
    "            if wordGold == wordPred:\r\n",
    "                cor += 1\r\n",
    "            if wordRaw != wordGold and wordPred == wordGold:\r\n",
    "                tp += 1\r\n",
    "            if wordRaw == wordGold and wordPred != wordGold:\r\n",
    "                fp += 1\r\n",
    "                with open(folder_path + file_path, 'a') as f:\r\n",
    "                    s = f\"FP| Raw: {' '.join(sentRaw)}: {wordRaw} -> {wordPred}\\n\"\r\n",
    "                    f.write(s)\r\n",
    "            if wordRaw != wordGold and wordPred != wordGold:\r\n",
    "                fn += 1\r\n",
    "                if wordRaw == wordPred:\r\n",
    "                    new_raws += 1\r\n",
    "                else:\r\n",
    "                    with open(folder_path + file_path, 'a') as f:\r\n",
    "                        s = f\"FN| Raw: {' '.join(sentRaw)}: {wordRaw} -> {wordPred}, Gold: {wordGold}\\n\"\r\n",
    "                        f.write(s)\r\n",
    "            elif verbose:\r\n",
    "                print(wordRaw, wordGold, wordPred)\r\n",
    "            total += 1\r\n",
    "\r\n",
    "    accuracy = cor / total\r\n",
    "    lai = (total - changed) / total\r\n",
    "    err = (accuracy - lai) / (1-lai)\r\n",
    "    recall = tp / (tp + fn)\r\n",
    "    precision = tp / (tp + fp)\r\n",
    "    f1 = 2 * precision * recall / (precision + recall)\r\n",
    "\r\n",
    "    print('Baseline acc.(LAI): {:.2f}'.format(lai * 100)) \r\n",
    "    print('Accuracy:           {:.2f}'.format(accuracy * 100)) \r\n",
    "    print('ERR:                {:.2f}'.format(err * 100))\r\n",
    "    print('Precision:          {:.2f}'.format(precision * 100))\r\n",
    "    print('Recall:             {:.2f}'.format(recall * 100))\r\n",
    "    print('F1:                 {:.2f}'.format(f1 * 100))\r\n",
    "    print(f\"Total: {total}, TP: {tp}, FP: {fp}, FN: {fn}, New words: {new_raws}\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Trying something with Transformers\r\n",
    "1. Find a candidate from the mapping, if its found, great\r\n",
    "2. Else, trying a Masked model to replace the word\r\n",
    "\r\n",
    "Tried it manually seems like will be a pretty bad approach.\r\n",
    "\r\n",
    "**But we can try something else,**\r\n",
    "1. We can have a spell checker, filter the raw word in the spell checker\r\n",
    "2. Then we can use the MASKed model of a transformer and *compare its best results with the result from the spell checker*"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "def loadNormData(path):\r\n",
    "    rawData = []\r\n",
    "    goldData = []\r\n",
    "    curSent = []\r\n",
    "\r\n",
    "    for line in open(path):\r\n",
    "        tok = line.strip().split('\\t')\r\n",
    "\r\n",
    "        if tok == [''] or tok == []:\r\n",
    "            rawData.append([x[0] for x in curSent])\r\n",
    "            goldData.append([x[1] for x in curSent])\r\n",
    "            curSent = []\r\n",
    "\r\n",
    "        else:\r\n",
    "            if len(tok) > 2:\r\n",
    "                err('erroneous input, line:\\n' + line + '\\n in file ' + path + ' contains more then two elements')\r\n",
    "            if len(tok) == 1:\r\n",
    "                tok.append('')\r\n",
    "            curSent.append(tok)\r\n",
    "\r\n",
    "    # in case file does not end with newline\r\n",
    "    if curSent != []:\r\n",
    "        rawData.append([x[0] for x in curSent])\r\n",
    "        goldData.append([x[1] for x in curSent])\r\n",
    "    return rawData, goldData"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "train_raw, train_gold = loadNormData('Data/multilexnorm/data/en/train.norm')\r\n",
    "dev_raw, dev_gold = loadNormData('Data/multilexnorm/data/en/dev.norm')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "counts = {}\r\n",
    "for sentRaw, sentGold in zip(train_raw, train_gold):\r\n",
    "    for wordRaw, wordGold in zip(sentRaw, sentGold):\r\n",
    "        if wordRaw not in counts:\r\n",
    "            counts[wordRaw] = {}\r\n",
    "        if wordGold not in counts[wordRaw]:\r\n",
    "            counts[wordRaw][wordGold] = 0\r\n",
    "        counts[wordRaw][wordGold] += 1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "from transformers import pipeline, AutoTokenizer"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\sidii\\anaconda3\\lib\\site-packages\\torchaudio\\extension\\extension.py:13: UserWarning: torchaudio C++ extension is not available.\n",
      "  warnings.warn('torchaudio C++ extension is not available.')\n",
      "C:\\Users\\sidii\\anaconda3\\lib\\site-packages\\torchaudio\\backend\\utils.py:89: UserWarning: No audio backend is available.\n",
      "  warnings.warn('No audio backend is available.')\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Using the Cardiff NLP `Twitter-roberta-base` Model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "MODEL = \"cardiffnlp/twitter-roberta-base\"\r\n",
    "fill_mask = pipeline(\"fill-mask\", model=MODEL, tokenizer=MODEL, device=0)\r\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Downloading: 100%|██████████| 565/565 [00:00<00:00, 548kB/s]\n",
      "Downloading: 100%|██████████| 501M/501M [02:14<00:00, 3.73MB/s]\n",
      "Downloading: 100%|██████████| 899k/899k [00:00<00:00, 1.24MB/s]\n",
      "Downloading: 100%|██████████| 456k/456k [00:00<00:00, 723kB/s]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Using the `BERT BASE Uncased` Model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "MODEL = \"bert-base-uncased\"\r\n",
    "unmasker = pipeline(\"fill-mask\", model=MODEL, device=0)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "def preprocess(text):\r\n",
    "    new_text = []\r\n",
    "    for t in text:\r\n",
    "        t = '@user' if t.startswith('@') and len(t) > 1 else t\r\n",
    "        t = 'http' if t.startswith('http') else t\r\n",
    "        new_text.append(t)\r\n",
    "    return \" \".join(new_text)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "def get_best_normalization(token, c_cmap, c_trs):\r\n",
    "    scores = {}\r\n",
    "\r\n",
    "    scores[token] = 1\r\n",
    "    if len(c_cmap) != 0:\r\n",
    "        for i in range(len(c_cmap)):  \r\n",
    "            scores[c_cmap[i]] = 3*(len(c_cmap) - i)\r\n",
    "\r\n",
    "    mul_factor = 2\r\n",
    "    for token in c_trs:\r\n",
    "        if token in scores.keys():\r\n",
    "            scores[token] *= mul_factor\r\n",
    "        \r\n",
    "    sorted_options = [norm for norm, f in sorted(scores.items(), key=lambda item: -item[1])]\r\n",
    "    return sorted_options[0]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "def normalize_v1(t):\r\n",
    "  '''Normalizing with conversion_map as well as some help from transformer model'''\r\n",
    "  norm = list.copy(t)\r\n",
    "\r\n",
    "  for ctr, token in enumerate(t):\r\n",
    "    # checking in the static map\r\n",
    "    c_cmap = []\r\n",
    "    if token in counts:\r\n",
    "      s = counts[token]\r\n",
    "      c_cmap = [norm for norm, f in sorted(s.items(), key=lambda item: -item[1])]\r\n",
    "\r\n",
    "    # checking options from transformer\r\n",
    "    # masked_text = t[:ctr] + ['[MASK]'] + t[(ctr + 1):]\r\n",
    "    # candidates = unmasker(preprocess(masked_text))\r\n",
    "    # c_trs = [c['token_str'].strip() for c in candidates]\r\n",
    "    c_trs = []\r\n",
    "   \r\n",
    "\r\n",
    "    # using all the resuts (token, c_cmap, c_trs, c_spc)\r\n",
    "    norm_token = get_best_normalization(token, c_cmap, c_trs)\r\n",
    "    norm[ctr] = norm_token\r\n",
    "\r\n",
    "  return norm"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "print(f\"raw: {dev_raw[0]}\")\r\n",
    "normalize_v1(dev_raw[0])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "raw: ['@cdutra5', 'bruh', 'get', 'out', 'yo', 'feelings', 'lol']\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['@cdutra5', 'brother', 'get', 'out', 'your', 'feelings', 'lol']"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "pred = []\r\n",
    "for raw in dev_raw:\r\n",
    "  pred.append(normalize_v1(raw))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### MFR Score (Baseline 2)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "evaluate(dev_raw, dev_gold, pred, \"mfr\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Baseline acc.(LAI): 93.10\n",
      "Accuracy:           97.37\n",
      "ERR:                61.93\n",
      "Precision:          91.88\n",
      "Recall:             67.93\n",
      "F1:                 78.11\n",
      "Total: 9169, TP: 430, FP: 38, FN: 203, New words: 190\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "MFR with the **new counts map**, which has only *conversions for words that should be normalized*. The mistake here is that it creates a lot of fase positives overfitting on the static map data. Hence its not a good way."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "source": [
    "evaluate(dev_raw, dev_gold, pred)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "raw: rt, pred: retweet\n",
      "raw: i, pred: in\n",
      "raw: were, pred: wear\n",
      "raw: rt, pred: retweet\n",
      "raw: k, pred: \n",
      "raw: a, pred: ass\n",
      "raw: rt, pred: retweet\n",
      "raw: rt, pred: retweet\n",
      "raw: 4, pred: for\n",
      "raw: da, pred: the\n",
      "raw: no, pred: know\n",
      "raw: every, pred: ever\n",
      "raw: rt, pred: retweet\n",
      "raw: be, pred: but\n",
      "raw: i, pred: in\n",
      "raw: m, pred: am\n",
      "raw: rt, pred: retweet\n",
      "raw: your, pred: you're\n",
      "raw: i, pred: in\n",
      "raw: a, pred: ass\n",
      "raw: i, pred: in\n",
      "raw: your, pred: you're\n",
      "raw: rt, pred: retweet\n",
      "raw: i, pred: in\n",
      "raw: i, pred: in\n",
      "raw: rt, pred: retweet\n",
      "raw: a, pred: ass\n",
      "raw: every, pred: ever\n",
      "raw: o, pred: off\n",
      "raw: be, pred: but\n",
      "raw: its, pred: it's\n",
      "raw: i, pred: in\n",
      "raw: ohhh, pred: oh\n",
      "raw: rt, pred: retweet\n",
      "raw: rt, pred: retweet\n",
      "raw: be, pred: but\n",
      "raw: a, pred: ass\n",
      "raw: i, pred: in\n",
      "raw: i, pred: in\n",
      "raw: order, pred: \n",
      "raw: a, pred: ass\n",
      "raw: rt, pred: retweet\n",
      "raw: 4, pred: for\n",
      "raw: rt, pred: retweet\n",
      "raw: i, pred: in\n",
      "raw: a, pred: ass\n",
      "raw: a, pred: ass\n",
      "raw: i, pred: in\n",
      "raw: i, pred: in\n",
      "raw: i, pred: in\n",
      "raw: rt, pred: retweet\n",
      "raw: e, pred: is\n",
      "raw: rt, pred: retweet\n",
      "raw: q, pred: question\n",
      "raw: i, pred: in\n",
      "raw: a, pred: ass\n",
      "raw: na, pred: no\n",
      "raw: rt, pred: retweet\n",
      "raw: rt, pred: retweet\n",
      "raw: n, pred: and\n",
      "raw: ft, pred: featuring\n",
      "raw: rt, pred: retweet\n",
      "raw: rt, pred: retweet\n",
      "raw: a, pred: ass\n",
      "raw: k, pred: \n",
      "raw: a, pred: ass\n",
      "raw: a, pred: ass\n",
      "raw: a, pred: ass\n",
      "raw: rt, pred: retweet\n",
      "raw: i, pred: in\n",
      "raw: i, pred: in\n",
      "raw: i, pred: in\n",
      "raw: rt, pred: retweet\n",
      "raw: w, pred: with\n",
      "raw: a, pred: ass\n",
      "raw: nah, pred: no\n",
      "raw: a, pred: ass\n",
      "raw: rt, pred: retweet\n",
      "raw: rt, pred: retweet\n",
      "raw: a, pred: ass\n",
      "raw: rt, pred: retweet\n",
      "raw: rt, pred: retweet\n",
      "raw: be, pred: but\n",
      "raw: i, pred: in\n",
      "raw: be, pred: but\n",
      "raw: a, pred: ass\n",
      "raw: rt, pred: retweet\n",
      "raw: i, pred: in\n",
      "raw: i, pred: in\n",
      "raw: i, pred: in\n",
      "raw: rt, pred: retweet\n",
      "raw: i, pred: in\n",
      "raw: i, pred: in\n",
      "raw: a, pred: ass\n",
      "raw: rt, pred: retweet\n",
      "raw: i, pred: in\n",
      "raw: your, pred: you're\n",
      "raw: i, pred: in\n",
      "raw: its, pred: it's\n",
      "raw: n, pred: and\n",
      "raw: rt, pred: retweet\n",
      "raw: rt, pred: retweet\n",
      "raw: its, pred: it's\n",
      "raw: i, pred: in\n",
      "raw: its, pred: it's\n",
      "raw: rt, pred: retweet\n",
      "raw: i, pred: in\n",
      "raw: rt, pred: retweet\n",
      "raw: rp, pred: real people\n",
      "raw: no, pred: know\n",
      "raw: f, pred: fuck\n",
      "raw: rt, pred: retweet\n",
      "raw: a, pred: ass\n",
      "raw: rt, pred: retweet\n",
      "raw: d, pred: the\n",
      "raw: rt, pred: retweet\n",
      "raw: i, pred: in\n",
      "raw: a, pred: ass\n",
      "raw: a, pred: ass\n",
      "raw: a, pred: ass\n",
      "raw: i, pred: in\n",
      "raw: i, pred: in\n",
      "raw: be, pred: but\n",
      "raw: a, pred: ass\n",
      "raw: a, pred: ass\n",
      "raw: rt, pred: retweet\n",
      "raw: i, pred: in\n",
      "raw: rt, pred: retweet\n",
      "raw: your, pred: you're\n",
      "raw: na, pred: no\n",
      "raw: no, pred: know\n",
      "raw: 4, pred: for\n",
      "raw: rt, pred: retweet\n",
      "raw: a, pred: ass\n",
      "raw: i, pred: in\n",
      "raw: rt, pred: retweet\n",
      "raw: a, pred: ass\n",
      "raw: i, pred: in\n",
      "raw: 2, pred: to\n",
      "raw: i, pred: in\n",
      "raw: thang, pred: thing\n",
      "raw: be, pred: but\n",
      "raw: s, pred: \n",
      "raw: i, pred: in\n",
      "raw: a, pred: ass\n",
      "raw: rt, pred: retweet\n",
      "raw: rt, pred: retweet\n",
      "raw: a, pred: ass\n",
      "raw: ish, pred: shit\n",
      "raw: i, pred: in\n",
      "raw: a, pred: ass\n",
      "raw: i, pred: in\n",
      "raw: ft, pred: featuring\n",
      "raw: i, pred: in\n",
      "raw: no, pred: know\n",
      "raw: rt, pred: retweet\n",
      "raw: rt, pred: retweet\n",
      "raw: a, pred: ass\n",
      "raw: merch, pred: merchandise\n",
      "raw: rt, pred: retweet\n",
      "raw: na, pred: no\n",
      "raw: a, pred: ass\n",
      "raw: no, pred: know\n",
      "raw: rt, pred: retweet\n",
      "raw: a, pred: ass\n",
      "raw: a, pred: ass\n",
      "raw: rt, pred: retweet\n",
      "raw: no, pred: know\n",
      "raw: rt, pred: retweet\n",
      "raw: no, pred: know\n",
      "raw: l, pred: laughing\n",
      "raw: m, pred: am\n",
      "raw: rt, pred: retweet\n",
      "raw: rt, pred: retweet\n",
      "raw: i, pred: in\n",
      "raw: your, pred: you're\n",
      "raw: a, pred: ass\n",
      "raw: rt, pred: retweet\n",
      "raw: o, pred: off\n",
      "raw: a, pred: ass\n",
      "raw: rt, pred: retweet\n",
      "raw: wont, pred: won't\n",
      "raw: i, pred: in\n",
      "raw: i, pred: in\n",
      "raw: rt, pred: retweet\n",
      "raw: a, pred: ass\n",
      "raw: rt, pred: retweet\n",
      "raw: i, pred: in\n",
      "raw: a, pred: ass\n",
      "raw: s, pred: \n",
      "raw: rt, pred: retweet\n",
      "raw: a, pred: ass\n",
      "raw: a, pred: ass\n",
      "raw: rt, pred: retweet\n",
      "raw: be, pred: but\n",
      "raw: a, pred: ass\n",
      "raw: rt, pred: retweet\n",
      "raw: no, pred: know\n",
      "raw: its, pred: it's\n",
      "raw: your, pred: you're\n",
      "raw: be, pred: but\n",
      "raw: rt, pred: retweet\n",
      "raw: a, pred: ass\n",
      "raw: rp, pred: real people\n",
      "raw: rt, pred: retweet\n",
      "raw: be, pred: but\n",
      "raw: i, pred: in\n",
      "raw: peewee, pred: pee wee\n",
      "raw: a, pred: ass\n",
      "raw: e, pred: is\n",
      "raw: p, pred: peace\n",
      "raw: i, pred: in\n",
      "raw: i, pred: in\n",
      "raw: a, pred: ass\n",
      "raw: ang, pred: and\n",
      "raw: rt, pred: retweet\n",
      "raw: rt, pred: retweet\n",
      "raw: d, pred: the\n",
      "raw: o, pred: off\n",
      "raw: shot, pred: \n",
      "raw: rt, pred: retweet\n",
      "raw: i, pred: in\n",
      "raw: a, pred: ass\n",
      "raw: rt, pred: retweet\n",
      "raw: i, pred: in\n",
      "raw: g, pred: girl\n",
      "raw: g, pred: girl\n",
      "raw: y, pred: why\n",
      "raw: i, pred: in\n",
      "raw: rt, pred: retweet\n",
      "raw: rt, pred: retweet\n",
      "raw: rt, pred: retweet\n",
      "raw: its, pred: it's\n",
      "raw: no, pred: know\n",
      "raw: a, pred: ass\n",
      "raw: i, pred: in\n",
      "raw: i, pred: in\n",
      "raw: i, pred: in\n",
      "raw: be, pred: but\n",
      "raw: ft, pred: featuring\n",
      "raw: i, pred: in\n",
      "raw: rt, pred: retweet\n",
      "raw: be, pred: but\n",
      "raw: be, pred: but\n",
      "raw: i, pred: in\n",
      "raw: no, pred: know\n",
      "raw: i, pred: in\n",
      "raw: i, pred: in\n",
      "raw: i, pred: in\n",
      "raw: ma, pred: my\n",
      "raw: a, pred: ass\n",
      "raw: your, pred: you're\n",
      "raw: a, pred: ass\n",
      "raw: its, pred: it's\n",
      "raw: rt, pred: retweet\n",
      "raw: rt, pred: retweet\n",
      "raw: rt, pred: retweet\n",
      "raw: i, pred: in\n",
      "raw: rt, pred: retweet\n",
      "raw: 2, pred: to\n",
      "raw: i, pred: in\n",
      "raw: rt, pred: retweet\n",
      "raw: rt, pred: retweet\n",
      "raw: i, pred: in\n",
      "raw: i, pred: in\n",
      "raw: i, pred: in\n",
      "raw: i, pred: in\n",
      "raw: rt, pred: retweet\n",
      "raw: rt, pred: retweet\n",
      "raw: rt, pred: retweet\n",
      "raw: em, pred: them\n",
      "raw: rt, pred: retweet\n",
      "raw: rt, pred: retweet\n",
      "raw: i, pred: in\n",
      "raw: order, pred: \n",
      "raw: order, pred: \n",
      "raw: t, pred: \n",
      "raw: dm, pred: direct message\n",
      "raw: rt, pred: retweet\n",
      "raw: a, pred: ass\n",
      "raw: rt, pred: retweet\n",
      "raw: 2, pred: to\n",
      "raw: rt, pred: retweet\n",
      "raw: rt, pred: retweet\n",
      "raw: rp, pred: real people\n",
      "raw: i, pred: in\n",
      "raw: rt, pred: retweet\n",
      "raw: your, pred: you're\n",
      "raw: rt, pred: retweet\n",
      "raw: a, pred: ass\n",
      "raw: s, pred: \n",
      "raw: no, pred: know\n",
      "raw: no, pred: know\n",
      "raw: e, pred: is\n",
      "raw: l, pred: laughing\n",
      "raw: i, pred: in\n",
      "raw: asap, pred: as soon as possible\n",
      "raw: be, pred: but\n",
      "raw: i, pred: in\n",
      "raw: no, pred: know\n",
      "raw: be, pred: but\n",
      "raw: be, pred: but\n",
      "raw: i, pred: in\n",
      "raw: i, pred: in\n",
      "raw: be, pred: but\n",
      "raw: a, pred: ass\n",
      "raw: a, pred: ass\n",
      "raw: a, pred: ass\n",
      "raw: rt, pred: retweet\n",
      "raw: i, pred: in\n",
      "raw: a, pred: ass\n",
      "raw: i, pred: in\n",
      "raw: rt, pred: retweet\n",
      "raw: o, pred: off\n",
      "raw: i, pred: in\n",
      "raw: i, pred: in\n",
      "raw: i, pred: in\n",
      "raw: rt, pred: retweet\n",
      "raw: a, pred: ass\n",
      "raw: rt, pred: retweet\n",
      "raw: rt, pred: retweet\n",
      "raw: i, pred: in\n",
      "raw: rt, pred: retweet\n",
      "raw: i, pred: in\n",
      "raw: your, pred: you're\n",
      "raw: ang, pred: and\n",
      "raw: rt, pred: retweet\n",
      "raw: no, pred: know\n",
      "raw: be, pred: but\n",
      "raw: rt, pred: retweet\n",
      "raw: i, pred: in\n",
      "raw: i, pred: in\n",
      "raw: no, pred: know\n",
      "raw: rt, pred: retweet\n",
      "raw: t, pred: \n",
      "raw: r, pred: are\n",
      "raw: 2, pred: to\n",
      "raw: weeknd, pred: weekend\n",
      "raw: rt, pred: retweet\n",
      "raw: no, pred: know\n",
      "raw: rt, pred: retweet\n",
      "raw: i, pred: in\n",
      "raw: rt, pred: retweet\n",
      "raw: your, pred: you're\n",
      "raw: be, pred: but\n",
      "raw: order, pred: \n",
      "raw: rt, pred: retweet\n",
      "raw: your, pred: you're\n",
      "raw: no, pred: know\n",
      "raw: rt, pred: retweet\n",
      "raw: mi, pred: me\n",
      "raw: a, pred: ass\n",
      "raw: i, pred: in\n",
      "raw: rt, pred: retweet\n",
      "raw: fu, pred: fuck\n",
      "raw: fu, pred: fuck\n",
      "raw: i, pred: in\n",
      "raw: rt, pred: retweet\n",
      "raw: d, pred: the\n",
      "raw: rt, pred: retweet\n",
      "raw: i, pred: in\n",
      "raw: your, pred: you're\n",
      "raw: rt, pred: retweet\n",
      "raw: i, pred: in\n",
      "raw: its, pred: it's\n",
      "raw: rt, pred: retweet\n",
      "raw: no, pred: know\n",
      "raw: i, pred: in\n",
      "raw: los, pred: las\n",
      "raw: rt, pred: retweet\n",
      "raw: i, pred: in\n",
      "raw: 2, pred: to\n",
      "raw: giveaway, pred: give away\n",
      "raw: 2, pred: to\n",
      "raw: rt, pred: retweet\n",
      "raw: a, pred: ass\n",
      "raw: rt, pred: retweet\n",
      "raw: rt, pred: retweet\n",
      "raw: rt, pred: retweet\n",
      "raw: rt, pred: retweet\n",
      "raw: be, pred: but\n",
      "raw: a, pred: ass\n",
      "raw: rt, pred: retweet\n",
      "raw: rp, pred: real people\n",
      "raw: i, pred: in\n",
      "raw: no, pred: know\n",
      "raw: fr, pred: from\n",
      "raw: i, pred: in\n",
      "raw: rt, pred: retweet\n",
      "raw: rt, pred: retweet\n",
      "raw: be, pred: but\n",
      "raw: your, pred: you're\n",
      "raw: rt, pred: retweet\n",
      "raw: na, pred: no\n",
      "raw: rt, pred: retweet\n",
      "raw: i, pred: in\n",
      "raw: no, pred: know\n",
      "raw: no, pred: know\n",
      "raw: rt, pred: retweet\n",
      "raw: rt, pred: retweet\n",
      "raw: i, pred: in\n",
      "raw: a, pred: ass\n",
      "raw: ofc, pred: of course\n",
      "raw: i, pred: in\n",
      "raw: rt, pred: retweet\n",
      "raw: i, pred: in\n",
      "raw: rt, pred: retweet\n",
      "raw: ang, pred: and\n",
      "raw: na, pred: no\n",
      "raw: i, pred: in\n",
      "raw: rt, pred: retweet\n",
      "raw: rp, pred: real people\n",
      "raw: nah, pred: no\n",
      "raw: be, pred: but\n",
      "raw: i, pred: in\n",
      "raw: nah, pred: no\n",
      "raw: rt, pred: retweet\n",
      "raw: no, pred: know\n",
      "raw: d, pred: the\n",
      "raw: rt, pred: retweet\n",
      "raw: i, pred: in\n",
      "raw: i, pred: in\n",
      "raw: rt, pred: retweet\n",
      "raw: i, pred: in\n",
      "raw: a, pred: ass\n",
      "raw: rt, pred: retweet\n",
      "raw: rp, pred: real people\n",
      "raw: rt, pred: retweet\n",
      "raw: harrystyles, pred: harry styles\n",
      "raw: rt, pred: retweet\n",
      "raw: rt, pred: retweet\n",
      "raw: i, pred: in\n",
      "raw: i, pred: in\n",
      "raw: i, pred: in\n",
      "raw: a, pred: ass\n",
      "raw: los, pred: las\n",
      "raw: a, pred: ass\n",
      "raw: rt, pred: retweet\n",
      "raw: a, pred: ass\n",
      "raw: m, pred: am\n",
      "raw: rt, pred: retweet\n",
      "raw: a, pred: ass\n",
      "raw: a, pred: ass\n",
      "raw: be, pred: but\n",
      "raw: i, pred: in\n",
      "raw: a, pred: ass\n",
      "raw: rt, pred: retweet\n",
      "raw: da, pred: the\n",
      "raw: a, pred: ass\n",
      "raw: m, pred: am\n",
      "raw: rt, pred: retweet\n",
      "raw: rp, pred: real people\n",
      "raw: rt, pred: retweet\n",
      "raw: a, pred: ass\n",
      "raw: nw, pred: no worries\n",
      "raw: rt, pred: retweet\n",
      "raw: your, pred: you're\n",
      "raw: ya, pred: you\n",
      "raw: a, pred: ass\n",
      "raw: i, pred: in\n",
      "raw: rt, pred: retweet\n",
      "raw: rt, pred: retweet\n",
      "raw: fav, pred: favorite\n",
      "raw: a, pred: ass\n",
      "raw: your, pred: you're\n",
      "raw: e, pred: is\n",
      "raw: rt, pred: retweet\n",
      "raw: a, pred: ass\n",
      "raw: no, pred: know\n",
      "raw: i, pred: in\n",
      "raw: ya, pred: you\n",
      "raw: no, pred: know\n",
      "raw: i, pred: in\n",
      "raw: i, pred: in\n",
      "raw: wont, pred: won't\n",
      "raw: i, pred: in\n",
      "raw: i, pred: in\n",
      "raw: rt, pred: retweet\n",
      "raw: a, pred: ass\n",
      "raw: i, pred: in\n",
      "raw: a, pred: ass\n",
      "raw: f, pred: fuck\n",
      "raw: e, pred: is\n",
      "raw: rt, pred: retweet\n",
      "raw: i, pred: in\n",
      "raw: a, pred: ass\n",
      "raw: t, pred: \n",
      "raw: a, pred: ass\n",
      "raw: rt, pred: retweet\n",
      "raw: i, pred: in\n",
      "raw: rt, pred: retweet\n",
      "raw: a, pred: ass\n",
      "raw: i, pred: in\n",
      "raw: d, pred: the\n",
      "raw: rt, pred: retweet\n",
      "raw: a, pred: ass\n",
      "raw: i, pred: in\n",
      "raw: i, pred: in\n",
      "raw: rt, pred: retweet\n",
      "raw: rt, pred: retweet\n",
      "raw: rp, pred: real people\n",
      "raw: i, pred: in\n",
      "raw: i, pred: in\n",
      "raw: o, pred: off\n",
      "raw: rt, pred: retweet\n",
      "raw: i, pred: in\n",
      "raw: i, pred: in\n",
      "raw: a, pred: ass\n",
      "raw: c, pred: see\n",
      "raw: 4, pred: for\n",
      "raw: i, pred: in\n",
      "raw: a, pred: ass\n",
      "raw: i, pred: in\n",
      "raw: a, pred: ass\n",
      "raw: be, pred: but\n",
      "raw: c, pred: see\n",
      "raw: rt, pred: retweet\n",
      "raw: gd, pred: good\n",
      "raw: rt, pred: retweet\n",
      "raw: a, pred: ass\n",
      "raw: i, pred: in\n",
      "raw: l, pred: laughing\n",
      "raw: o, pred: off\n",
      "raw: l, pred: laughing\n",
      "raw: yh, pred: yeah\n",
      "raw: no, pred: know\n",
      "raw: bc, pred: because\n",
      "raw: a, pred: ass\n",
      "raw: a, pred: ass\n",
      "raw: i, pred: in\n",
      "raw: i, pred: in\n",
      "raw: rt, pred: retweet\n",
      "raw: rt, pred: retweet\n",
      "raw: rt, pred: retweet\n",
      "raw: be, pred: but\n",
      "raw: your, pred: you're\n",
      "raw: rt, pred: retweet\n",
      "raw: usa, pred: use\n",
      "raw: rt, pred: retweet\n",
      "raw: rt, pred: retweet\n",
      "raw: i, pred: in\n",
      "raw: rt, pred: retweet\n",
      "raw: a, pred: ass\n",
      "raw: i, pred: in\n",
      "raw: i, pred: in\n",
      "raw: rt, pred: retweet\n",
      "raw: a, pred: ass\n",
      "raw: i, pred: in\n",
      "raw: your, pred: you're\n",
      "raw: your, pred: you're\n",
      "raw: i, pred: in\n",
      "raw: i, pred: in\n",
      "raw: rt, pred: retweet\n",
      "raw: a, pred: ass\n",
      "raw: were, pred: wear\n",
      "raw: ft, pred: featuring\n",
      "raw: rt, pred: retweet\n",
      "raw: rt, pred: retweet\n",
      "raw: i, pred: in\n",
      "raw: were, pred: wear\n",
      "raw: a, pred: ass\n",
      "raw: rt, pred: retweet\n",
      "raw: screen, pred: screenshot\n",
      "raw: rt, pred: retweet\n",
      "raw: screen, pred: screenshot\n",
      "raw: be, pred: but\n",
      "raw: rt, pred: retweet\n",
      "raw: a, pred: ass\n",
      "raw: i, pred: in\n",
      "raw: ish, pred: shit\n",
      "raw: rt, pred: retweet\n",
      "raw: a, pred: ass\n",
      "raw: b, pred: be\n",
      "raw: i, pred: in\n",
      "raw: rt, pred: retweet\n",
      "raw: your, pred: you're\n",
      "raw: a, pred: ass\n",
      "raw: i, pred: in\n",
      "raw: rt, pred: retweet\n",
      "raw: usa, pred: use\n",
      "raw: rt, pred: retweet\n",
      "raw: rt, pred: retweet\n",
      "raw: craic, pred: crack\n",
      "raw: a, pred: ass\n",
      "raw: be, pred: but\n",
      "raw: rt, pred: retweet\n",
      "raw: pics, pred: pictures\n",
      "raw: i, pred: in\n",
      "raw: i, pred: in\n",
      "raw: i, pred: in\n",
      "raw: i, pred: in\n",
      "raw: a, pred: ass\n",
      "raw: rt, pred: retweet\n",
      "raw: be, pred: but\n",
      "raw: a, pred: ass\n",
      "raw: rt, pred: retweet\n",
      "raw: its, pred: it's\n",
      "raw: rt, pred: retweet\n",
      "raw: i, pred: in\n",
      "raw: rt, pred: retweet\n",
      "raw: i, pred: in\n",
      "raw: rip, pred: rest in peace\n",
      "raw: a, pred: ass\n",
      "raw: your, pred: you're\n",
      "raw: your, pred: you're\n",
      "raw: a, pred: ass\n",
      "raw: no, pred: know\n",
      "raw: i, pred: in\n",
      "raw: i, pred: in\n",
      "raw: t, pred: \n",
      "raw: t, pred: \n",
      "raw: no, pred: know\n",
      "raw: no, pred: know\n",
      "raw: rt, pred: retweet\n",
      "raw: a, pred: ass\n",
      "raw: en, pred: in\n",
      "raw: babe, pred: baby\n",
      "raw: dey, pred: they\n",
      "raw: rt, pred: retweet\n",
      "raw: ma, pred: my\n",
      "raw: q, pred: question\n",
      "raw: its, pred: it's\n",
      "raw: head, pred: headache\n",
      "raw: rt, pred: retweet\n",
      "raw: rt, pred: retweet\n",
      "raw: a, pred: ass\n",
      "raw: rt, pred: retweet\n",
      "raw: hoe, pred: whore\n",
      "Baseline acc.(LAI): 93.10\n",
      "Accuracy:           91.25\n",
      "ERR:                -26.70\n",
      "Precision:          42.23\n",
      "Recall:             72.51\n",
      "F1:                 53.37\n",
      "Total: 9169, TP: 459, FP: 628, FN: 174, New words: 158\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Making some hyperparameter changes so as to improve the score with the transformer"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Results with using the `BERT-BASE-UNCASED` Model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "evaluate(dev_raw, dev_gold, pred, \"trs_mf2\") #mul_factor = 2"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Baseline acc.(LAI): 93.10\n",
      "Accuracy:           97.40\n",
      "ERR:                62.40\n",
      "Precision:          91.58\n",
      "Recall:             68.72\n",
      "F1:                 78.52\n",
      "Total: 9169, TP: 435, FP: 40, FN: 198, New words: 187\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "evaluate(dev_raw, dev_gold, pred_new, \"trs_x2_mf2\") # running the normalisation on the semi-normalised batch"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Baseline acc.(LAI): 93.10\n",
      "Accuracy:           97.38\n",
      "ERR:                62.09\n",
      "Precision:          91.54\n",
      "Recall:             68.40\n",
      "F1:                 78.30\n",
      "Total: 9169, TP: 433, FP: 40, FN: 200, New words: 186\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "source": [
    "evaluate(dev_raw, dev_gold, pred) #mul_factor = 3"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Baseline acc.(LAI): 93.10\n",
      "Accuracy:           97.21\n",
      "ERR:                59.56\n",
      "Precision:          91.07\n",
      "Recall:             66.03\n",
      "F1:                 76.56\n",
      "Total: 9169, TP: 418, FP: 41, FN: 215\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Increasing the `mul_factor` we can see that the "
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.8",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "2476b43a1381252f3e4867362f308f1b5866abd3c80db7d3bc84a9d7006e407b"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}